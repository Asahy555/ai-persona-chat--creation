# ===========================================
# КОНФИГУРАЦИЯ ДЛЯ ЛОКАЛЬНЫХ ИИ БЕЗ ЦЕНЗУРЫ
# ===========================================

# ============================================
# 1. OLLAMA (ЛОКАЛЬНЫЙ LLM - РЕКОМЕНДУЕТСЯ)
# ============================================
# Установка: https://ollama.com
# 1. Скачайте и установите Ollama
# 2. Запустите: ollama serve
# 3. Загрузите модель без цензуры:
#    ollama pull llama2-uncensored
#    или
#    ollama pull dolphin-mistral
#    или
#    ollama pull wizard-vicuna-uncensored

OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2-uncensored

# Другие рекомендуемые модели:
# - dolphin-mistral (лучше для кодирования и русского языка)
# - dolphin-mixtral (самое высокое качество, требует 16GB+ VRAM)
# - wizard-vicuna-uncensored (стабильная, проверенная)

# ============================================
# 2. GIGACHAT (СБЕРБАНК - РАБОТАЕТ В РОССИИ)
# ============================================
# Получение ключей: https://developers.sber.ru/portal/products/gigachat
# 1. Зарегистрируйтесь/войдите
# 2. Создайте приложение в Developer Portal
# 3. Скопируйте CLIENT_ID и CLIENT_SECRET
# Freemium лимиты: 900,000 токенов/год бесплатно

GIGACHAT_CLIENT_ID=your_client_id_here
GIGACHAT_CLIENT_SECRET=your_client_secret_here

# ============================================
# 3. OPENROUTER (АЛЬТЕРНАТИВА, РАБОТАЕТ ИЗ РФ)
# ============================================
# Регистрация: https://openrouter.ai
# Бесплатные кредиты $5 при регистрации
# Поддерживает uncensored модели через API

OPENROUTER_API_KEY=sk-or-v1-your-key-here

# ============================================
# 4. STABLE DIFFUSION (ЛОКАЛЬНАЯ ГЕНЕРАЦИЯ)
# ============================================

# --- Вариант A: Fooocus (ЛЕГЧЕ, РЕКОМЕНДУЕТСЯ) ---
# Установка: https://github.com/lllyasviel/Fooocus
# 1. git clone https://github.com/lllyasviel/Fooocus.git
# 2. cd Fooocus
# 3. python launch.py
# Откроется на http://localhost:7860

FOOOCUS_API_URL=http://localhost:7860

# --- Вариант B: AUTOMATIC1111 (ПРОДВИНУТЫЙ) ---
# Установка: https://github.com/AUTOMATIC1111/stable-diffusion-webui
# 1. git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
# 2. cd stable-diffusion-webui
# 3. ./webui-user.bat --api  (или .sh на Linux/Mac)
# Откроется на http://localhost:7860

SD_WEBUI_URL=http://localhost:7860

# ============================================
# ПРИОРИТЕТЫ FALLBACK (АВТОМАТИЧЕСКИ)
# ============================================
# Текст: Ollama → GigaChat → OpenRouter → Fallback
# Изображения: Fooocus → AUTOMATIC1111 → Fallback (заглушка)

# ============================================
# ИНСТРУКЦИИ ПО УСТАНОВКЕ
# ============================================

# ШАГ 1: Установите Ollama (для текстовых ответов)
# Windows/Mac: Скачайте с https://ollama.com
# Linux: curl -fsSL https://ollama.com/install.sh | sh
# Затем: ollama serve
# И: ollama pull llama2-uncensored

# ШАГ 2: Установите Fooocus (для генерации изображений)
# git clone https://github.com/lllyasviel/Fooocus.git
# cd Fooocus
# python launch.py

# ШАГ 3: (Опционально) Настройте GigaChat для резерва
# Зарегистрируйтесь на https://developers.sber.ru
# Получите CLIENT_ID и CLIENT_SECRET
# Добавьте в .env.local

# ============================================
# СИСТЕМНЫЕ ТРЕБОВАНИЯ
# ============================================
# Ollama + llama2-uncensored: 8GB RAM, 4GB VRAM (минимум)
# Ollama + dolphin-mistral: 8GB RAM, 6GB VRAM
# Fooocus: 8GB RAM, 6GB VRAM (NVIDIA GPU)
# AUTOMATIC1111: 8GB RAM, 8GB VRAM (NVIDIA GPU)

# Для CPU-only (без GPU): используйте только GigaChat или OpenRouter
